{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232becfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:54:17.548897Z",
     "start_time": "2021-06-20T05:54:17.544730Z"
    }
   },
   "source": [
    "# å‚è€ƒç½‘å€\n",
    "https://www.kaggle.com/arnabs007/pretrain-a-bert-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0792a5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:52:52.378125Z",
     "start_time": "2021-06-20T05:52:52.374572Z"
    }
   },
   "source": [
    "# è¯­æ–™æ•°æ®\n",
    "ç”¨äºè®­ç»ƒbertæ¨¡å‹çš„è¯­æ–™æ•°æ®ï¼Œå¸¸è§çš„å¤§è§„æ¨¡æ•°æ®åœ¨datasetsé‡Œé¢éƒ½å¯ä»¥ç›´æ¥ä¸‹è½½å¹¶åŠ è½½ï¼Œè¯¦ç»†è¯·å‚è€ƒèµ„æ–™https://huggingface.co/docs/datasets/index.htmlã€‚å¯¹äºè‡ªå·±çš„è¯­æ–™æ•°æ®ï¼Œæˆ–è€…è„±æ•çš„æ•°æ®ï¼Œ\n",
    "æˆ‘ä»¬éœ€è¦è‡ªå·±å¤„ç†ä¸‹ï¼Œè¿™é‡Œå»ºè®®æ¯ä¸€è¡Œä½œä¸ºä¸€ä¸ªå¥å­ï¼Œè¯ä¸è¯ä¹‹é—´æœ€å¥½æœ‰åˆ†éš”ç¬¦."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef2ba75d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T08:43:23.867436Z",
     "start_time": "2021-06-20T08:43:19.158637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (1.9.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from torch) (3.7.4.3)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tokenizers in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (0.10.3)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: transformers in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (4.7.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from transformers) (4.5.0)\n",
      "Requirement already satisfied: requests in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from transformers) (4.49.0)\n",
      "Requirement already satisfied: filelock in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: sacremoses in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: pyyaml in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from transformers) (1.20.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: joblib in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install tokenizers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af0a54",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "You can use your own text corpus or you can download one from OSCAR, these are huge multilingual corpora obtained by language classification and filtering of Common Crawl dumps of the Web.\n",
    "\n",
    "One thing to keep in mind, you will get better results by pretraining your data on more and more data.\n",
    "\n",
    "If you are using your own corpus, make sure that your text corpus is one sentence-per-line like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e0888",
   "metadata": {},
   "source": [
    "```\n",
    "Mr. Cassius crossed the highway, and stopped suddenly.\n",
    "Something glittered in the nearest red pool before him.\n",
    "Gold, surely!\n",
    "But, wonderful to relate, not an irregular, shapeless fragment of crude ore, fresh from Nature's crucible.\n",
    "Looking at it more attentively, he saw that it bore the inscription, \"May to Cass.\"\n",
    "Like most of his fellow gold-seekers, Cass was superstitious.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3b755",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "We will have to train our own tokenizer and build a vocabulary for our corpus. We will be choosing BertWordPieceTokenizer from tokenizers library. Arbitrarily choose a vocab_size=50,000. The model will be saved to the output directory as 'name-vocab.txt' file.\n",
    "\n",
    "I had a pretrained tokenizer for Bangla, so I am using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb0a6eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T05:57:10.590420Z",
     "start_time": "2021-06-20T05:57:10.553194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/bert-bangla-vocab.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a tokenizer\n",
    "import tokenizers\n",
    " \n",
    "bwpt = tokenizers.BertWordPieceTokenizer()\n",
    " \n",
    "filepath = \"./data/raw_bangla_for_BERT.txt\"\n",
    "\n",
    "bwpt.train(\n",
    "    files=[filepath],\n",
    "    vocab_size=50000,\n",
    "    min_frequency=3,\n",
    "    limit_alphabet=1000\n",
    ")\n",
    "\n",
    "bwpt.save_model('./data','bert-bangla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5bd561e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T08:43:32.173043Z",
     "start_time": "2021-06-20T08:43:32.095659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['à¦¶à§‡à¦·', 'à¦¦à¦¿à¦•à§‡', 'à¦¸à§‡à¦¨à¦¾à¦¬à¦¾à¦¹à¦¿à¦¨à§€à¦°', 'à¦¸à¦¦à¦¸à¦¯à¦°à¦¾', 'à¦à¦¸à¦¬', 'à¦˜à¦°', 'à¦¤à¦¾à¦°', 'à¦ªà¦°à¦¶à¦¾à¦¸à¦¨à§‡à¦°', 'à¦•à¦¾à¦›à§‡', 'à¦¹à¦¸à¦¤à¦¾à¦¨à¦¤à¦°', 'à¦•à¦°à§‡à¦¨']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1631: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "from transformers import BertTokenizer, LineByLineTextDataset\n",
    "\n",
    "vocab_file_dir = './data/bert-bangla-vocab.txt' \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_file_dir)\n",
    "\n",
    "sentence = 'à¦¶à§‡à¦· à¦¦à¦¿à¦•à§‡ à¦¸à§‡à¦¨à¦¾à¦¬à¦¾à¦¹à¦¿à¦¨à§€à¦° à¦¸à¦¦à¦¸à§à¦¯à¦°à¦¾ à¦à¦¸à¦¬ à¦˜à¦° à¦¤à¦¾à¦à¦° à¦ªà§à¦°à¦¶à¦¾à¦¸à¦¨à§‡à¦° à¦•à¦¾à¦›à§‡ à¦¹à¦¸à§à¦¤à¦¾à¦¨à§à¦¤à¦° à¦•à¦°à§‡à¦¨'\n",
    "\n",
    "encoded_input = tokenizer.tokenize(sentence)\n",
    "print(encoded_input)\n",
    "# print(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89517524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T08:57:48.531384Z",
     "start_time": "2021-06-20T08:43:34.227712Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lhj/anaconda3/envs/d2l/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of lines:  2172033\n"
     ]
    }
   ],
   "source": [
    "# some bugs for LineByLineTextDataset https://discuss.huggingface.co/t/how-to-train-a-language-model-from-scratch-when-my-dataset-is-bigger-than-ram/117\n",
    "'''\n",
    "transformers has a predefined class LineByLineTextDataset()\n",
    "which reads your text line by line and converts them to tokens\n",
    "'''\n",
    "\n",
    "dataset= LineByLineTextDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    file_path = './data/raw_bangla_for_BERT.txt',\n",
    "    block_size = 128  # maximum sequence length\n",
    ")\n",
    "\n",
    "print('No. of lines: ', len(dataset)) # No of lines in your datset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded17e1",
   "metadata": {},
   "source": [
    "## Defining model\n",
    "Now that have the training data ready to be fed into the model, let's define the model. First we have to define the configuration of the BERT model. vocab_size should be the size of your trained vocabulary. Keep the rest of the arguments as they are. I am expecting that you have a thorough knowledge on the transformers model to understand the parameters\n",
    "\n",
    "We will be using BertForMaskedLM from transformers library which is built on top of masked language modelling(MLM) excluding the next sentence prediction(NSP) task.\n",
    "\n",
    "You also need to define a DataCollator. What is DataCollator you ask?\n",
    "\n",
    "A DataCollator is a function that takes a list of samples from a Dataset and collate them into a batch, as a dictionary of Tensors.\n",
    "\n",
    "collates batches of tensors, honoring their tokenizer's pad_token\n",
    "preprocesses batches for masked language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98c4e166",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T09:01:50.950155Z",
     "start_time": "2021-06-20T09:01:49.235672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of parameters:  81965648\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=50000,\n",
    "    hidden_size=768, \n",
    "    num_hidden_layers=6, \n",
    "    num_attention_heads=12,\n",
    "    max_position_embeddings=512\n",
    ")\n",
    " \n",
    "model = BertForMaskedLM(config)\n",
    "print('No of parameters: ', model.num_parameters())\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c420eda",
   "metadata": {},
   "source": [
    "## Defining training arguments\n",
    "per_device_train_batch_size is theoretically not the same as the batch size for BERT model. This is true when you have more than 1 GPU/TPU.\n",
    "\n",
    "But as of now in practicality, assuming that you are training the model on 1 GPU(In colab/your pc) per_device_train_batch_size is the bach size for your BERT model, which is I have set 32 (recommended batch size for BERT in the paper =16 or 32).\n",
    "\n",
    "Then instantiate a trainer with the predefined model, tokenizer, datacollator and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10ac5e8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T09:03:33.450620Z",
     "start_time": "2021-06-20T09:03:33.441502Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd952dda",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "We are at the last step of our language model pretraining. Call the trainer's train() method and sit back and watch a movie cause this is going to take a lot of time depending on your corpus size.\n",
    "\n",
    "Remember Google's BERT-base was trained on 4 cloud TPUs for 4 uninterrupted days. That is equivalent to 16 GPU days!\n",
    "\n",
    "I trained a model on a random newspaper article corpus of only 500MB containing around 2.2M sentences and 30M words and that took almost 4 hrs!\n",
    "\n",
    "Don't forget to save the model! Cause you know, if you fall asleep (I am certain you will) and wake up and see runtime disconnected! RIP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9f841",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-20T09:03:42.067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='434' max='67877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  434/67877 1:02:07 < 161:37:47, 0.12 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()\n",
    "trainer.save_model('/kaggle/working/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4c8195",
   "metadata": {},
   "source": [
    "## Check your model's prediction\n",
    "Load your pretained model from the saved model directory and a make a pipeline for masked word prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c90ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('/kaggle/working/')\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask('à¦²à¦¾à¦¶ à¦‰à¦¦à§à¦§à¦¾à¦° à¦•à¦°à§‡ à¦®à¦¯à¦¼à¦¨à¦¾à¦¤à¦¦à¦¨à§à¦¤à§‡à¦° à¦œà¦¨à§à¦¯ à¦•à¦•à§à¦¸à¦¬à¦¾à¦œà¦¾à¦° [MASK] à¦®à¦°à§à¦—à§‡ à¦ªà¦¾à¦ à¦¿à¦¯à¦¼à§‡à¦›à§‡ à¦ªà§à¦²à¦¿à¦¶')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632477c",
   "metadata": {},
   "source": [
    "## ç»“è®º\n",
    "æˆ‘çš„æ¨¡å‹åšå¾—ç›¸å½“ä¸é”™ æ­£å¦‚æˆ‘å…ˆå‰æ‰€è¯´ï¼ŒBERTéœ€è¦å¤§é‡çš„æ–‡æœ¬æ¥æ›´å¥½åœ°ç†è§£ä¸€ç§è¯­è¨€ã€‚è°·æ­Œçš„BERT-baseæ˜¯åœ¨åŒ…å«çº¦33äº¿ä¸ªå•è¯çš„TeraBytesåŸå§‹æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒçš„ï¼ˆå¤§çº¦æ˜¯æˆ‘ä»¬è®­ç»ƒçš„110å€ï¼‰ã€‚\n",
    "\n",
    "æˆ‘åœ¨éšæœºçš„æŠ¥çº¸æ–‡ç« ä¸Šè®­ç»ƒæˆ‘çš„æ¨¡å‹ã€‚ä¸ºä½ çš„ä»»åŠ¡åœ¨ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬ä¸Šè®­ç»ƒä½ çš„BERTæ¨¡å‹ä¼šæ›´å¥½ã€‚åœ¨é‚£ä¸ªé¢†åŸŸï¼Œä½ è‚¯å®šä¼šå¾—åˆ°æ›´å¥½çš„ç»“æœã€‚\n",
    "\n",
    "æ‰€ä»¥ï¼Œæ­å–œä½ ï¼ä½ ç°åœ¨å¯ä»¥è®­ç»ƒä½ è‡ªå·±çš„BERTæ¨¡å‹äº†ã€‚ä½ ç°åœ¨å¯ä»¥åœ¨ä»»ä½•è¯­è¨€ä¸­è®­ç»ƒä½ è‡ªå·±çš„BERTæ¨¡å‹ã€‚\n",
    "\n",
    "ç°åœ¨ï¼Œä½ çš„è„‘æµ·ä¸­å¯èƒ½ä¼šå‡ºç°ä¸€ä¸ªé—®é¢˜ã€‚\n",
    "\n",
    "æˆ‘å¯ä»¥é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„æ¨¡å‹çš„æƒé‡æ¥è®­ç»ƒä¸€ä¸ªæ¨¡å‹å—ï¼Ÿ\n",
    "\n",
    "æ˜¯çš„ï¼Œä½ å¯ä»¥ã€‚æ³¨æ„åœ¨æ¨¡å‹å®šä¹‰éƒ¨åˆ†ï¼Œæˆ‘ä»¥è¿™ç§æ–¹å¼å®šä¹‰äº†æ¨¡å‹ã€‚\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "\n",
    "è¿™é‡ŒBertConfigè¢«ä½œä¸ºå‚æ•°ä¼ é€’ï¼Œè€Œä½ è¦åšçš„æ˜¯model = BertForMaskedLM.from_pretained('bert-base-cased')\n",
    "\n",
    "æˆ–è€…å¦‚æœä½ æƒ³ä»æœ¬åœ°ç›®å½•åŠ è½½æ¨¡å‹ï¼Œmodel = BertForMaskedLM.from_pretained('your_model_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc235df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "a54e23490e26d1a483616a4f12e07d8a7daa8529b25c63f16540f56747dc94bc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
